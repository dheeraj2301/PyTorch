{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout(0.10)\n",
    "        self.dp2 = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        loss = F.nll_loss(model(X), y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset),\n",
    "        100. * success / len(test_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_folder():\n",
    "    current_dir = Path.cwd()\n",
    "    # parent_dir = current_dir.parent\n",
    "    data_folder = current_dir / 'data'\n",
    "    data_folder.mkdir(exist_ok=True)\n",
    "create_data_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:35<00:00, 281312.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 129179.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:03<00:00, 445605.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 4533681.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "                datasets.MNIST('data',\n",
    "                                train=True,\n",
    "                                download=True,\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1302,),(0.3069,))\n",
    "                                ])),\n",
    "                batch_size=32, shuffle=True\n",
    "                )\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "                datasets.MNIST('data',\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1302,),(0.3069,))\n",
    "                                ])),\n",
    "                batch_size=500, shuffle=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.311839\n",
      "epoch: 1 [320/60000 (1%)]\t training loss: 1.866824\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 1.082228\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 0.880526\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 0.838555\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 0.496232\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.436633\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.394346\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 0.454175\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.388366\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.496029\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.295328\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.427557\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.564096\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.360088\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.614586\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.166789\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.310312\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.110744\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.173897\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.235618\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.084721\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.389549\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.035540\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.201249\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.248436\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 0.237274\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.219494\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.083085\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.281732\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.241079\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.285707\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.174618\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.508552\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.072515\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.124014\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.199905\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.080617\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.207695\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.055134\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.086084\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.087330\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.117902\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.096953\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.021222\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.126288\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.128389\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.349406\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.154229\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.107494\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.186217\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.161257\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.043976\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.242164\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 0.069929\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.023108\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.019210\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.021838\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.103363\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.368642\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.062395\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.060089\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.091186\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.182622\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.061457\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.113717\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.317567\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.124547\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.048010\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.057886\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.163891\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.258241\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.055966\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.187243\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.173890\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.167412\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.068031\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.114713\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.052942\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.017010\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.298596\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 0.345197\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.092701\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.033430\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.142079\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.032793\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.009788\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.100683\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.009509\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.183290\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.096341\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.357328\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.038421\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.034778\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 0.590650\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.162004\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.089410\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.096829\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.108234\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.017902\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.005612\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.232163\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.224961\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.009375\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.270053\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.169644\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.037525\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.021778\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.004147\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.109729\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.239624\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.083625\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.010437\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.079997\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.057987\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.194336\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.045120\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.112574\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.051832\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.100241\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.019106\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 0.388259\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.062825\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.225306\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.081811\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.168576\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.058640\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.403822\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.127535\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.123907\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.144330\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.011248\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.004192\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.024738\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.080484\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.017783\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.239072\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.029078\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.116029\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.074885\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.149390\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.038613\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.126141\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.006669\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.206370\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.030288\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.128630\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.114716\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.073700\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 0.154025\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.112291\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.178926\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.045252\n",
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.021192\n",
      "epoch: 1 [49280/60000 (82%)]\t training loss: 0.150413\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.045478\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.155041\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.045723\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.004457\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.009748\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.198831\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.018214\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.006931\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.025610\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.014884\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.053373\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.009145\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.053131\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.053376\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.003719\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.168846\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.005717\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.004250\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.074199\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.041139\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.081942\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.044569\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.015013\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.084457\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.084835\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.006300\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.109339\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.052982\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.012009\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.060294\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.003358\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.021780\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.038727\n",
      "\n",
      "Test dataset: Overall Loss: 0.0498, Overall Accuracy: 9841/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.066778\n",
      "epoch: 2 [320/60000 (1%)]\t training loss: 0.042087\n",
      "epoch: 2 [640/60000 (1%)]\t training loss: 0.196033\n",
      "epoch: 2 [960/60000 (2%)]\t training loss: 0.014928\n",
      "epoch: 2 [1280/60000 (2%)]\t training loss: 0.018155\n",
      "epoch: 2 [1600/60000 (3%)]\t training loss: 0.006605\n",
      "epoch: 2 [1920/60000 (3%)]\t training loss: 0.048299\n",
      "epoch: 2 [2240/60000 (4%)]\t training loss: 0.021497\n",
      "epoch: 2 [2560/60000 (4%)]\t training loss: 0.181214\n",
      "epoch: 2 [2880/60000 (5%)]\t training loss: 0.013777\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.067961\n",
      "epoch: 2 [3520/60000 (6%)]\t training loss: 0.063495\n",
      "epoch: 2 [3840/60000 (6%)]\t training loss: 0.026865\n",
      "epoch: 2 [4160/60000 (7%)]\t training loss: 0.002393\n",
      "epoch: 2 [4480/60000 (7%)]\t training loss: 0.042499\n",
      "epoch: 2 [4800/60000 (8%)]\t training loss: 0.049833\n",
      "epoch: 2 [5120/60000 (9%)]\t training loss: 0.180580\n",
      "epoch: 2 [5440/60000 (9%)]\t training loss: 0.157667\n",
      "epoch: 2 [5760/60000 (10%)]\t training loss: 0.048058\n",
      "epoch: 2 [6080/60000 (10%)]\t training loss: 0.068367\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.055677\n",
      "epoch: 2 [6720/60000 (11%)]\t training loss: 0.026296\n",
      "epoch: 2 [7040/60000 (12%)]\t training loss: 0.036512\n",
      "epoch: 2 [7360/60000 (12%)]\t training loss: 0.002071\n",
      "epoch: 2 [7680/60000 (13%)]\t training loss: 0.022990\n",
      "epoch: 2 [8000/60000 (13%)]\t training loss: 0.167630\n",
      "epoch: 2 [8320/60000 (14%)]\t training loss: 0.230569\n",
      "epoch: 2 [8640/60000 (14%)]\t training loss: 0.027113\n",
      "epoch: 2 [8960/60000 (15%)]\t training loss: 0.033427\n",
      "epoch: 2 [9280/60000 (15%)]\t training loss: 0.007574\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.051545\n",
      "epoch: 2 [9920/60000 (17%)]\t training loss: 0.001675\n",
      "epoch: 2 [10240/60000 (17%)]\t training loss: 0.028276\n",
      "epoch: 2 [10560/60000 (18%)]\t training loss: 0.000850\n",
      "epoch: 2 [10880/60000 (18%)]\t training loss: 0.026882\n",
      "epoch: 2 [11200/60000 (19%)]\t training loss: 0.010885\n",
      "epoch: 2 [11520/60000 (19%)]\t training loss: 0.007659\n",
      "epoch: 2 [11840/60000 (20%)]\t training loss: 0.092954\n",
      "epoch: 2 [12160/60000 (20%)]\t training loss: 0.031369\n",
      "epoch: 2 [12480/60000 (21%)]\t training loss: 0.056641\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.006817\n",
      "epoch: 2 [13120/60000 (22%)]\t training loss: 0.205160\n",
      "epoch: 2 [13440/60000 (22%)]\t training loss: 0.117094\n",
      "epoch: 2 [13760/60000 (23%)]\t training loss: 0.027046\n",
      "epoch: 2 [14080/60000 (23%)]\t training loss: 0.025355\n",
      "epoch: 2 [14400/60000 (24%)]\t training loss: 0.020384\n",
      "epoch: 2 [14720/60000 (25%)]\t training loss: 0.019279\n",
      "epoch: 2 [15040/60000 (25%)]\t training loss: 0.004988\n",
      "epoch: 2 [15360/60000 (26%)]\t training loss: 0.118969\n",
      "epoch: 2 [15680/60000 (26%)]\t training loss: 0.035990\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.082506\n",
      "epoch: 2 [16320/60000 (27%)]\t training loss: 0.143645\n",
      "epoch: 2 [16640/60000 (28%)]\t training loss: 0.003158\n",
      "epoch: 2 [16960/60000 (28%)]\t training loss: 0.242778\n",
      "epoch: 2 [17280/60000 (29%)]\t training loss: 0.042844\n",
      "epoch: 2 [17600/60000 (29%)]\t training loss: 0.122744\n",
      "epoch: 2 [17920/60000 (30%)]\t training loss: 0.072313\n",
      "epoch: 2 [18240/60000 (30%)]\t training loss: 0.058820\n",
      "epoch: 2 [18560/60000 (31%)]\t training loss: 0.005768\n",
      "epoch: 2 [18880/60000 (31%)]\t training loss: 0.065931\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.129147\n",
      "epoch: 2 [19520/60000 (33%)]\t training loss: 0.002291\n",
      "epoch: 2 [19840/60000 (33%)]\t training loss: 0.096243\n",
      "epoch: 2 [20160/60000 (34%)]\t training loss: 0.067510\n",
      "epoch: 2 [20480/60000 (34%)]\t training loss: 0.083706\n",
      "epoch: 2 [20800/60000 (35%)]\t training loss: 0.015779\n",
      "epoch: 2 [21120/60000 (35%)]\t training loss: 0.120213\n",
      "epoch: 2 [21440/60000 (36%)]\t training loss: 0.020487\n",
      "epoch: 2 [21760/60000 (36%)]\t training loss: 0.032544\n",
      "epoch: 2 [22080/60000 (37%)]\t training loss: 0.008371\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.028046\n",
      "epoch: 2 [22720/60000 (38%)]\t training loss: 0.023739\n",
      "epoch: 2 [23040/60000 (38%)]\t training loss: 0.085613\n",
      "epoch: 2 [23360/60000 (39%)]\t training loss: 0.077393\n",
      "epoch: 2 [23680/60000 (39%)]\t training loss: 0.045967\n",
      "epoch: 2 [24000/60000 (40%)]\t training loss: 0.015023\n",
      "epoch: 2 [24320/60000 (41%)]\t training loss: 0.113203\n",
      "epoch: 2 [24640/60000 (41%)]\t training loss: 0.042363\n",
      "epoch: 2 [24960/60000 (42%)]\t training loss: 0.026863\n",
      "epoch: 2 [25280/60000 (42%)]\t training loss: 0.098856\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.017342\n",
      "epoch: 2 [25920/60000 (43%)]\t training loss: 0.019213\n",
      "epoch: 2 [26240/60000 (44%)]\t training loss: 0.000951\n",
      "epoch: 2 [26560/60000 (44%)]\t training loss: 0.016645\n",
      "epoch: 2 [26880/60000 (45%)]\t training loss: 0.153577\n",
      "epoch: 2 [27200/60000 (45%)]\t training loss: 0.408600\n",
      "epoch: 2 [27520/60000 (46%)]\t training loss: 0.022586\n",
      "epoch: 2 [27840/60000 (46%)]\t training loss: 0.009023\n",
      "epoch: 2 [28160/60000 (47%)]\t training loss: 0.021580\n",
      "epoch: 2 [28480/60000 (47%)]\t training loss: 0.177932\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.011115\n",
      "epoch: 2 [29120/60000 (49%)]\t training loss: 0.005646\n",
      "epoch: 2 [29440/60000 (49%)]\t training loss: 0.079347\n",
      "epoch: 2 [29760/60000 (50%)]\t training loss: 0.124157\n",
      "epoch: 2 [30080/60000 (50%)]\t training loss: 0.022944\n",
      "epoch: 2 [30400/60000 (51%)]\t training loss: 0.002811\n",
      "epoch: 2 [30720/60000 (51%)]\t training loss: 0.027020\n",
      "epoch: 2 [31040/60000 (52%)]\t training loss: 0.203212\n",
      "epoch: 2 [31360/60000 (52%)]\t training loss: 0.015889\n",
      "epoch: 2 [31680/60000 (53%)]\t training loss: 0.002788\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.003428\n",
      "epoch: 2 [32320/60000 (54%)]\t training loss: 0.002878\n",
      "epoch: 2 [32640/60000 (54%)]\t training loss: 0.035554\n",
      "epoch: 2 [32960/60000 (55%)]\t training loss: 0.003106\n",
      "epoch: 2 [33280/60000 (55%)]\t training loss: 0.176959\n",
      "epoch: 2 [33600/60000 (56%)]\t training loss: 0.034357\n",
      "epoch: 2 [33920/60000 (57%)]\t training loss: 0.012781\n",
      "epoch: 2 [34240/60000 (57%)]\t training loss: 0.350420\n",
      "epoch: 2 [34560/60000 (58%)]\t training loss: 0.005994\n",
      "epoch: 2 [34880/60000 (58%)]\t training loss: 0.023181\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.006059\n",
      "epoch: 2 [35520/60000 (59%)]\t training loss: 0.016785\n",
      "epoch: 2 [35840/60000 (60%)]\t training loss: 0.005265\n",
      "epoch: 2 [36160/60000 (60%)]\t training loss: 0.035305\n",
      "epoch: 2 [36480/60000 (61%)]\t training loss: 0.038840\n",
      "epoch: 2 [36800/60000 (61%)]\t training loss: 0.002971\n",
      "epoch: 2 [37120/60000 (62%)]\t training loss: 0.487281\n",
      "epoch: 2 [37440/60000 (62%)]\t training loss: 0.009430\n",
      "epoch: 2 [37760/60000 (63%)]\t training loss: 0.098061\n",
      "epoch: 2 [38080/60000 (63%)]\t training loss: 0.032068\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.015927\n",
      "epoch: 2 [38720/60000 (65%)]\t training loss: 0.156587\n",
      "epoch: 2 [39040/60000 (65%)]\t training loss: 0.028122\n",
      "epoch: 2 [39360/60000 (66%)]\t training loss: 0.007189\n",
      "epoch: 2 [39680/60000 (66%)]\t training loss: 0.050765\n",
      "epoch: 2 [40000/60000 (67%)]\t training loss: 0.006161\n",
      "epoch: 2 [40320/60000 (67%)]\t training loss: 0.139635\n",
      "epoch: 2 [40640/60000 (68%)]\t training loss: 0.597710\n",
      "epoch: 2 [40960/60000 (68%)]\t training loss: 0.017144\n",
      "epoch: 2 [41280/60000 (69%)]\t training loss: 0.015229\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.172482\n",
      "epoch: 2 [41920/60000 (70%)]\t training loss: 0.007006\n",
      "epoch: 2 [42240/60000 (70%)]\t training loss: 0.004921\n",
      "epoch: 2 [42560/60000 (71%)]\t training loss: 0.081308\n",
      "epoch: 2 [42880/60000 (71%)]\t training loss: 0.004191\n",
      "epoch: 2 [43200/60000 (72%)]\t training loss: 0.003387\n",
      "epoch: 2 [43520/60000 (73%)]\t training loss: 0.032008\n",
      "epoch: 2 [43840/60000 (73%)]\t training loss: 0.407140\n",
      "epoch: 2 [44160/60000 (74%)]\t training loss: 0.103758\n",
      "epoch: 2 [44480/60000 (74%)]\t training loss: 0.002457\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.002361\n",
      "epoch: 2 [45120/60000 (75%)]\t training loss: 0.140614\n",
      "epoch: 2 [45440/60000 (76%)]\t training loss: 0.141109\n",
      "epoch: 2 [45760/60000 (76%)]\t training loss: 0.006090\n",
      "epoch: 2 [46080/60000 (77%)]\t training loss: 0.036236\n",
      "epoch: 2 [46400/60000 (77%)]\t training loss: 0.044679\n",
      "epoch: 2 [46720/60000 (78%)]\t training loss: 0.119274\n",
      "epoch: 2 [47040/60000 (78%)]\t training loss: 0.033483\n",
      "epoch: 2 [47360/60000 (79%)]\t training loss: 0.071137\n",
      "epoch: 2 [47680/60000 (79%)]\t training loss: 0.089361\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.126149\n",
      "epoch: 2 [48320/60000 (81%)]\t training loss: 0.137836\n",
      "epoch: 2 [48640/60000 (81%)]\t training loss: 0.133502\n",
      "epoch: 2 [48960/60000 (82%)]\t training loss: 0.023539\n",
      "epoch: 2 [49280/60000 (82%)]\t training loss: 0.001278\n",
      "epoch: 2 [49600/60000 (83%)]\t training loss: 0.032089\n",
      "epoch: 2 [49920/60000 (83%)]\t training loss: 0.149945\n",
      "epoch: 2 [50240/60000 (84%)]\t training loss: 0.018385\n",
      "epoch: 2 [50560/60000 (84%)]\t training loss: 0.000791\n",
      "epoch: 2 [50880/60000 (85%)]\t training loss: 0.004940\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.026366\n",
      "epoch: 2 [51520/60000 (86%)]\t training loss: 0.172611\n",
      "epoch: 2 [51840/60000 (86%)]\t training loss: 0.025812\n",
      "epoch: 2 [52160/60000 (87%)]\t training loss: 0.001341\n",
      "epoch: 2 [52480/60000 (87%)]\t training loss: 0.171433\n",
      "epoch: 2 [52800/60000 (88%)]\t training loss: 0.154769\n",
      "epoch: 2 [53120/60000 (89%)]\t training loss: 0.001860\n",
      "epoch: 2 [53440/60000 (89%)]\t training loss: 0.090802\n",
      "epoch: 2 [53760/60000 (90%)]\t training loss: 0.097438\n",
      "epoch: 2 [54080/60000 (90%)]\t training loss: 0.008806\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 0.309339\n",
      "epoch: 2 [54720/60000 (91%)]\t training loss: 0.097001\n",
      "epoch: 2 [55040/60000 (92%)]\t training loss: 0.011131\n",
      "epoch: 2 [55360/60000 (92%)]\t training loss: 0.129750\n",
      "epoch: 2 [55680/60000 (93%)]\t training loss: 0.009648\n",
      "epoch: 2 [56000/60000 (93%)]\t training loss: 0.010522\n",
      "epoch: 2 [56320/60000 (94%)]\t training loss: 0.003310\n",
      "epoch: 2 [56640/60000 (94%)]\t training loss: 0.057307\n",
      "epoch: 2 [56960/60000 (95%)]\t training loss: 0.007876\n",
      "epoch: 2 [57280/60000 (95%)]\t training loss: 0.090604\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.070631\n",
      "epoch: 2 [57920/60000 (97%)]\t training loss: 0.084471\n",
      "epoch: 2 [58240/60000 (97%)]\t training loss: 0.037420\n",
      "epoch: 2 [58560/60000 (98%)]\t training loss: 0.001825\n",
      "epoch: 2 [58880/60000 (98%)]\t training loss: 0.004585\n",
      "epoch: 2 [59200/60000 (99%)]\t training loss: 0.032529\n",
      "epoch: 2 [59520/60000 (99%)]\t training loss: 0.010859\n",
      "epoch: 2 [59840/60000 (100%)]\t training loss: 0.024866\n",
      "\n",
      "Test dataset: Overall Loss: 0.0410, Overall Accuracy: 9855/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device('cpu')\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)\n",
    "\n",
    "for epoch in range(1,3):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1aa9a79dbe0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample_data[1][0],\n",
    "           cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is : 2\n",
      "Ground Truth is : 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][1]}\")\n",
    "print(f\"Ground Truth is : {sample_targets[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODEL = \"./covnet.pth\"\n",
    "torch.save(model.state_dict(), PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
